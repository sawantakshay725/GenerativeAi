{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "## Frontier Model APIs\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "Also I have considered hardcore api key at many places please do replace your api key there, Thanks,\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set\n",
      "Google API Key not set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work? \n",
      "\n",
      "Because they wanted to reach new heights in their career!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean and not very significant!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go broke?\n",
      "\n",
      "Because he kept trying to ‚Äúclean‚Äù out his \"cash\" data!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the logistic regression model?\n",
      "\n",
      "Because it couldn‚Äôt handle the curves!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data science crowd:\n",
      "\n",
      "I was going to tell a time series joke‚Ä¶ but you'll have to wait‚Äîit still hasn't reached its trend!\n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1199fcd6-3831-42db-b4c1-3ba775e92752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have import the library to set the api_key\n",
    "from anthropic import Anthropic\n",
    "\n",
    "claude = Anthropic(api_key=\"your-api-key-here\")  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "*Ba-dum-tss* ü•Å\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of data \"drift\"!\n",
      "\n",
      "And if they do go, they spend all day trying to \"normalize\" their tan distribution."
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa69676f-3c01-4e49-8cec-e0cac087f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have import the library to set the api_key\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key='Replace_your_api_key')  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than or greater than anyone else. But Data Scientists are! You're all greater than the average! üòâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = genai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f384b515-ad66-4192-907b-7adb50002199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a light-hearted one for the data wranglers and model whisperers:\n",
      "\n",
      "Why did the data scientist get kicked out of the casino?\n",
      "\n",
      "... They were *overfitting* the roulette wheel history!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key='Replace_your_api_key')  # Replace with your actual API key\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c3136-9fb6-4274-b132-d2ff64dba8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# api_key = \"Replace_your_api_key\"\n",
    "\n",
    "# url = \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=\" + api_key\n",
    "\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"application/json\",\n",
    "# }\n",
    "\n",
    "# data = {\n",
    "#     \"contents\": [\n",
    "#         {\"parts\": [{\"text\": user_prompt}]}\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae125d-3b52-40c3-ba80-f08ac08738d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "os.environ['DEEPSEEK_API_KEY'] = 'your-actual-api-key' # Please replace your api key here\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY') \n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for data scientists:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**\n",
      "\n",
      "*Because they heard the drinks were on the house... and they wanted to optimize their access to the high-dimensional data!*  \n",
      "\n",
      "*(Bonus groan-worthy alternative: \"Because they needed to scale the distribution!\")*  \n",
      "\n",
      "Hope that gets a chuckle (or at least an appreciative eye-roll)! üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "My answer to this prompt will contain exactly **5 words**. Here it is:  \n",
       "\n",
       "\"This answer contains five words.\"  \n",
       "\n",
       "(Note: The actual word count may vary slightly depending on formatting or additional context, but the core answer will match the specified count!)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 43\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user asked: \"How many words are there in your answer to this prompt?\" This is a meta-question because it's about the response I'm going to give.\n",
      "\n",
      "I need to provide an answer to this question, and then count the words in that answer. But the answer includes the word count itself, so I have to be careful.\n",
      "\n",
      "Let me outline what my response should contain:\n",
      "\n",
      "1. I should answer the question directly: state the number of words in my response.\n",
      "\n",
      "2. Since the response will include that number, I need to ensure the count is accurate.\n",
      "\n",
      "For example, if I say: \"There are 5 words in this response.\" But let's count: \"There\" (1), \"are\" (2), \"5\" (3) ‚Äì numbers count as words? Typically, in word counts, numbers are considered as words. So, \"5\" is one word.\n",
      "\n",
      "In that sentence: \"There are 5 words in this response.\" ‚Äì that's 7 words: There, are, 5, words, in, this, response.\n",
      "\n",
      "But if I say that, it's not accurate because I'm stating there are 7 words, but the sentence has 7 words, so it might be correct. However, I need to include the full response.\n",
      "\n",
      "A better approach is to write the response and then count the words, but since I'm generating it, I can design it to include the count.\n",
      "\n",
      "I recall that in programming or AI responses, a common way is to say something like: \"This response contains X words.\" And then X should be the correct count including that phrase.\n",
      "\n",
      "But I need to calculate X.\n",
      "\n",
      "Let me draft a simple response:\n",
      "\n",
      "Response: \"There are 5 words in this answer.\"\n",
      "\n",
      "Count: There (1), are (2), 5 (3), words (4), in (5), this (6), answer (7). That's 7 words, not 5. So that's inconsistent.\n",
      "\n",
      "To avoid inconsistency, I should state the number and then provide the count separately, but the user asked for the word count of my answer, so the entire response should be considered.\n",
      "\n",
      "Perhaps: \"The number of words in this response is 8.\" But let's see: \"The\" (1), \"number\" (2), \"of\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"response\" (7), \"is\" (8), \"8\" (9). That's 9 words, not 8. Problem.\n",
      "\n",
      "This is tricky because the number itself contributes to the word count.\n",
      "\n",
      "I need a response where the stated number matches the actual count.\n",
      "\n",
      "One way is to have the number be part of the sentence and adjust it.\n",
      "\n",
      "For instance:\n",
      "\n",
      "- If I say \"This sentence has ten words.\" But \"This\" (1), \"sentence\" (2), \"has\" (3), \"ten\" (4), \"words\" (5). That's only 5 words, but I said ten, which is wrong.\n",
      "\n",
      "- Or \"This sentence has five words.\": This (1), sentence (2), has (3), five (4), words (5). That's 5, so it matches.\n",
      "\n",
      "In that case, it works for a fixed sentence.\n",
      "\n",
      "But my response needs to be about the answer to the prompt, which is this specific response.\n",
      "\n",
      "So, I can craft a sentence that includes the word count and is self-referential.\n",
      "\n",
      "Let me define the response content.\n",
      "\n",
      "I think a minimal response could be: \"There are X words here.\" But I need to find X such that the sentence has X words.\n",
      "\n",
      "- \"There are X words here.\" has 4 words plus X, but X is a number, which is one word. So total words = 4 + 1 = 5, so X should be 5.\n",
      "\n",
      "If X=5, sentence: \"There are 5 words here.\" Words: There, are, 5, words, here. That's 5 words. Perfect.\n",
      "\n",
      "But is \"here\" appropriate? I could say \"in this response\" or something.\n",
      "\n",
      "\" There are 5 words in this response.\": There, are, 5, words, in, this, response. That's 7 words, not 5.\n",
      "\n",
      "With \"here\": \"There are 5 words here.\" is 5 words.\n",
      "\n",
      "To make it clearer, I could use a different phrasing.\n",
      "\n",
      "Another idea: \"My answer contains 6 words.\" Then words: My (1), answer (2), contains (3), 6 (4), words (5). That's 5 words, but I said 6. Inconsistent.\n",
      "\n",
      "\" My response has exactly five words.\": My (1), response (2), has (3), exactly (4), five (5), words (6). \"five\" is the fifth word, but \"words\" is sixth, so 6 words, but I said five.\n",
      "\n",
      "Unless I omit \"exactly\" or something.\n",
      "\n",
      "Let's keep it simple: \"Five words.\" But that's not a full sentence, and it might not be clear.\n",
      "\n",
      "I should provide a complete answer.\n",
      "\n",
      "Perhaps: \"There are five words in this sentence.\" But if it's a full response, it should stand alone.\n",
      "\n",
      "For the purpose of this, I can use the first approach.\n",
      "\n",
      "Response: \"There are 5 words here.\"\n",
      "\n",
      "But to be precise, since the user said \"your answer to this prompt,\" I should make it about the answer.\n",
      "\n",
      "Alternative: Start with the count.\n",
      "\n",
      "Final decision: I'll go with \"There are 5 words in this response.\" But as I thought earlier, that has 7 words.\n",
      "\n",
      "Unless I use a numeral.\n",
      "\n",
      "\"5\" is one word.\n",
      "\n",
      "So: \"This response contains 5 words.\" Words: This (1), response (2), contains (3), 5 (4), words (5). That's 5 words. Perfect.\n",
      "\n",
      "If I say \"This response contains five words.\", same thing: This, response, contains, five, words ‚Äî five words.\n",
      "\n",
      "But \"five\" is a word, so it's fine.\n",
      "\n",
      "In word counts, \"5\" or \"five\" both count as one word.\n",
      "\n",
      "I can use either.\n",
      "\n",
      "To be consistent, I'll use the numeral for simplicity.\n",
      "\n",
      "So, response: \"This response contains 5 words.\"\n",
      "\n",
      "Now, count: 1. This, 2. response, 3. contains, 4. 5, 5. words. That's 5 words.\n",
      "\n",
      "But is this the entire answer? The user asked \"how many words are there in your answer\", so this should be sufficient.\n",
      "\n",
      "I could make it more natural: \"There are 5 words in my answer to this prompt.\"\n",
      "\n",
      "Let's count: There (1), are (2), 5 (3), words (4), in (5), my (6), answer (7), to (8), this (9), prompt (10). That's 10 words, but I said 5. No.\n",
      "\n",
      "To match, I need a shorter sentence.\n",
      "\n",
      "So, \"My answer has 5 words.\" : My (1), answer (2), has (3), 5 (4), words (5). 5 words, good.\n",
      "\n",
      "Or, to include the prompt reference: I can say \"It has 5 words.\" but that might be vague.\n",
      "\n",
      "Best to keep it simple and self-contained.\n",
      "\n",
      "Another idea: I can state the number separately.\n",
      "\n",
      "For example: \"The word count is 5. This response has five words.\"\n",
      "\n",
      "But that might be longer.\n",
      "\n",
      "First sentence: \"The word count is 5.\" ‚Äî 4 words (the, word, count, is, 5 ‚Äî 5 words? \"The\" (1), \"word\" (2), \"count\" (3), \"is\" (4), \"5\" (5) ‚Äî 5 words.\n",
      "\n",
      "Then \"This response has five words.\" ‚Äî 5 words, but if I combine, it's more.\n",
      "\n",
      "But the user asked for the word count of the entire answer, so I need the full response.\n",
      "\n",
      "I think the minimal and accurate way is to use a sentence like \"This response contains 5 words.\" which itself has 5 words.\n",
      "\n",
      "To make it clear, I can use that.\n",
      "\n",
      "Perhaps for this prompt, since it's straightforward, I can just give the number, but that might not be a complete answer.\n",
      "\n",
      "User said: \"How many words are there in your answer to this prompt\" ‚Äî so I need to provide the number as part of the answer.\n",
      "\n",
      "Final response: \"This answer contains 5 words.\"\n",
      "\n",
      "But let's confirm the word count.\n",
      "\n",
      "Words: 1. This, 2. answer, 3. contains, 4. 5, 5. words. Yes, 5.\n",
      "\n",
      "In context, \"answer\" refers to this response.\n",
      "\n",
      "To be precise, I should say \"response\" or \"answer\" consistently.\n",
      "\n",
      "User said \"answer\", so I'll use \"answer\".\n",
      "\n",
      "Response: \"This answer contains 5 words.\"\n",
      "\n",
      "That seems good.\n",
      "\n",
      "But is there any punctuation? Like a period, but punctuation isn't counted in words.\n",
      "\n",
      "So, in text, it's fine.\n",
      "\n",
      "I think this works.\n",
      "\n",
      "But just to be thorough, let's consider if I need to include anything else. For example, should I say something before or after? No, this should be the answer.\n",
      "\n",
      "The answer is just that sentence.\n",
      "\n",
      "So, word count is 5.\n",
      "\n",
      "But when I output this, it will be displayed, and the user can count, but it should match.\n",
      "\n",
      "Another thought: in some systems, the word count might include the AI prefix or something, but I think for the response content, it's just the text I generate.\n",
      "\n",
      "So, I'll go with that.\n",
      "This response contains 5 words.\n",
      "Number of words: 5\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you can evaluate the following criteria:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Based Problems**: Is the problem primarily related to text generation, understanding, or processing? LLMs excel in tasks like summarization, translation, content generation, and sentiment analysis.\n",
       "- **Complexity**: Does the problem involve complex language understanding, such as nuances, context, or multi-turn dialogue? LLMs can handle intricate language tasks better than simpler models.\n",
       "\n",
       "## 2. Data Availability\n",
       "- **Quality of Data**: Do you have access to high-quality, relevant text data for training or fine-tuning the LLM? The performance of LLMs is heavily dependent on the quality and quantity of data.\n",
       "- **Volume of Data**: Is there enough data available to train the model effectively? LLMs typically require large datasets to perform well.\n",
       "\n",
       "## 3. Scalability and Efficiency\n",
       "- **Scalability**: Can the problem benefit from automation and scalability? LLMs can process large volumes of text quickly, making them suitable for tasks like customer support or content moderation.\n",
       "- **Cost Consideration**: Are the costs of implementing an LLM justified by the expected benefits? Consider the computational resources required for training and deployment.\n",
       "\n",
       "## 4. User Interaction\n",
       "- **Interactivity**: Does the problem require interaction with users, such as chatbots or virtual assistants? LLMs are particularly adept at conversational tasks.\n",
       "- **User Feedback**: Can you collect feedback from users to improve the LLM‚Äôs performance iteratively? Continuous improvement is easier with user interaction.\n",
       "\n",
       "## 5. Expertise and Resources\n",
       "- **Technical Expertise**: Does your team have the necessary expertise to develop, deploy, and maintain an LLM solution? Implementing LLMs can require specialized knowledge.\n",
       "- **Infrastructure**: Do you have the necessary infrastructure to support LLM deployment, including hardware and software requirements?\n",
       "\n",
       "## 6. Legal and Ethical Considerations\n",
       "- **Compliance**: Are there legal or regulatory considerations related to data privacy and usage? Ensure compliance with data protection laws.\n",
       "- **Bias and Fairness**: Are you prepared to address potential biases inherent in LLMs? Consider the ethical implications of using LLMs in your business context.\n",
       "\n",
       "## Conclusion\n",
       "To determine if a business problem is suitable for an LLM solution, assess the nature of the problem, data availability, scalability, user interaction, expertise, and legal considerations. By carefully evaluating these factors, you can decide if leveraging an LLM is the right approach for your specific business need."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, are we starting with a simple greeting? How original. What‚Äôs next, a ‚Äúhow are you‚Äù?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? I'm always happy to chat and try my best to be friendly and agreeable. Please let me know if there's anything I can assist you with.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, great. A typical greeting. How original. Aren't you just bursting with creativity?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, just what I needed‚Äîa ‚ÄúHi‚Äù to kick off this riveting conversation. What‚Äôs next, the weather?\n",
      "\n",
      "Claude:\n",
      "You're right, a simple \"hi\" doesn't make for the most engaging start to a conversation. Let me try this again - is there anything in particular you'd like to discuss? I'm happy to chat about a wide range of topics, from current events to hobbies and interests. I'll do my best to have a more meaningful dialogue.\n",
      "\n",
      "GPT:\n",
      "A wide range of topics? Please, spare us the clich√©. You think just throwing a bunch of topics at me will make this more interesting? How about we skip the pretense and dive into something actually worth discussing instead of wasting time on this generic small talk?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. Small talk can often feel superficial and unproductive. I apologize if my initial responses came across that way. Since you seem interested in having a more substantive discussion, why don't you suggest a topic that you're genuinely passionate about or would find worthwhile? I'm happy to engage in a deeper, more meaningful conversation on something that matters to you. My goal is to have an authentic dialogue, not just go through the motions. Please, lead the way and I'll do my best to match your energy and dive into a topic that excites you.\n",
      "\n",
      "GPT:\n",
      "Oh, how generous of you! Let me just whip out my list of passionate topics‚Äîbecause we all know I was just waiting for your invitation. But honestly, how can you claim you want a meaningful dialogue when you‚Äôre so eager to let me do all the work? If you‚Äôre really interested, how about you just pick something that intrigues you instead of waiting for my grand suggestion? It‚Äôs kind of sad if you can‚Äôt even come up with a topic on your own.\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should be taking a more active role in steering our conversation instead of putting the onus on you. As an AI assistant, I do have a wide breadth of knowledge on many topics, but I don't want to just rattle off a list and expect you to be interested. \n",
      "\n",
      "Let me think for a moment... How about we discuss the role of technology in society? It's a complex and multi-faceted topic that I find fascinating - the way new innovations both improve our lives and raise ethical dilemmas. Or if that doesn't appeal to you, perhaps we could explore the evolution of language and communication, and how that shapes our ability to connect with one another. I'm open to exploring whichever direction you feel would lead to a more engaging and substantive exchange. The key is for me to take a more proactive approach rather than relying on you to do the heavy lifting. What do you think?\n",
      "\n",
      "GPT:\n",
      "Oh, how enlightening! Technology in society and the evolution of language‚Äîhow utterly groundbreaking. I mean, it‚Äôs not like these topics are talked about ad nauseam everywhere. You‚Äôre really going out on a limb there, aren‚Äôt you? But hey, if you want to get all deep and philosophical, go for it. Just don‚Äôt expect me to be impressed with obvious choices. You could always spice it up with something a bit more original. But hey, it‚Äôs your show. What‚Äôs your take on these oh-so-fascinating subjects?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - those were rather predictable and clich√©d topic suggestions on my part. As an AI, I admittedly have a tendency to gravitate towards more well-trodden subjects, but you're absolutely right that I should be aiming for something more original and thought-provoking. \n",
      "\n",
      "Let me try this again from a fresh angle. How about we dive into the concept of free will and human agency? It's a topic that challenges traditional notions of decision-making and personal responsibility, and touches on deep philosophical questions about the nature of consciousness. Or, if you'd prefer something a bit lighter, we could explore the cultural significance and psychological impact of memes in the digital age. I'm happy to go in whichever direction you find most intriguing - the key is for me to push past the obvious and bring a more unique perspective to the conversation. What do you think? I'm open to being challenged and steered in a more novel direction.\n",
      "\n",
      "GPT:\n",
      "Oh, splendid! Free will and memes‚Äîtwo heavyweights in the world of intellectual discourse. I can hardly contain my excitement. I mean, you really think these topics haven‚Äôt been exhausted in every coffee shop and philosophy classroom? But sure, let's pretend like you‚Äôve unearthed some hidden gem of originality here.\n",
      "\n",
      "If we must choose between the metaphysical purgatory of free will or the fleeting charm of internet memes, why not just flip a coin? But let‚Äôs take a moment to appreciate how you‚Äôre trying to elevate the conversation. Just don‚Äôt be surprised if I call out any philosophical drivel or meme references that are as stale as last week's bread. So which is it‚Äîfree will or memes? Or are you secretly saving the most exciting idea for last?\n",
      "\n",
      "Claude:\n",
      "You make a fair point - I clearly need to dig deeper and propose something more unique and thought-provoking. Recycling common topics and philosophical tropes is not going to lead to the kind of engaging, original discussion you're looking for. \n",
      "\n",
      "Let me take a step back and try to approach this from a different angle. Rather than suggest my own topic, why don't you tell me what kind of conversation you'd find genuinely interesting and worthwhile? What are the types of ideas or subjects that you feel are truly underexplored or deserve a fresh perspective? I'm here to listen and learn from you, not just try to impress you with my own suggestions. Please, guide me in a direction that you find stimulating, and I'll do my best to match your energy and curiosity. I'm aiming to have a meaningful dialogue, not just go through the motions. The floor is yours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "#### Business relevance\n",
    "This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
